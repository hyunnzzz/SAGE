# pdf_processor.py - PDF ì²˜ë¦¬ ì „ìš© ëª¨ë“ˆ

import pdfplumber
import numpy as np
import os
import glob
import json
import hashlib
from sklearn.metrics.pairwise import cosine_similarity

class PDFProcessor:
    def __init__(self, client):
        """
        PDF ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            client: Hugging Face InferenceClient
        """
        self.client = client
        self.chunks = []
        self.embeddings = []
        self.cache_dir = "cache"  # ìºì‹œ ë””ë ‰í† ë¦¬
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
            print(f"ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.cache_dir}")
    
    def _get_folder_hash(self, folder_path):
        """í´ë” ë‚´ PDF íŒŒì¼ë“¤ì˜ í•´ì‹œê°’ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
        pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
        pdf_files.sort()  # ìˆœì„œ ì¼ì •í•˜ê²Œ
        
        hash_string = ""
        for pdf_file in pdf_files:
            # íŒŒì¼ëª… + íŒŒì¼ í¬ê¸° + ìˆ˜ì • ì‹œê°„
            stat = os.stat(pdf_file)
            hash_string += f"{pdf_file}_{stat.st_size}_{stat.st_mtime}"
        
        return hashlib.md5(hash_string.encode()).hexdigest()
    
    def _save_cache(self, folder_path, chunks, embeddings):
        """ìºì‹œ ì €ì¥"""
        folder_hash = self._get_folder_hash(folder_path)
        
        # ì²­í¬ ì €ì¥
        chunks_file = os.path.join(self.cache_dir, f"chunks_{folder_hash}.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        # ì„ë² ë”© ì €ì¥
        embeddings_file = os.path.join(self.cache_dir, f"embeddings_{folder_hash}.npy")
        np.save(embeddings_file, np.array(embeddings))
        
        print(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {folder_hash}")
    
    def _load_cache(self, folder_path):
        """ìºì‹œ ë¡œë“œ"""
        folder_hash = self._get_folder_hash(folder_path)
        
        chunks_file = os.path.join(self.cache_dir, f"chunks_{folder_hash}.json")
        embeddings_file = os.path.join(self.cache_dir, f"embeddings_{folder_hash}.npy")
        
        # ìºì‹œ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not (os.path.exists(chunks_file) and os.path.exists(embeddings_file)):
            return False, None, None
        
        try:
            # ì²­í¬ ë¡œë“œ
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            # ì„ë² ë”© ë¡œë“œ
            embeddings = np.load(embeddings_file).tolist()
            
            print(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {len(embeddings)}ê°œ ì„ë² ë”©")
            return True, chunks, embeddings
            
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False, None, None
        
    def extract_text_from_pdf(self, pdf_path):
        """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        text = ""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                print(f"PDF ì´ í˜ì´ì§€ ìˆ˜: {len(pdf.pages)}")
                
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                        text += f"\n--- í˜ì´ì§€ {i+1} ---\n"
                        text += page_text + "\n"
                        
                print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
                return text
                
        except Exception as e:
            print(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
            return None

    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """
        í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ê° ì²­í¬ì˜ í¬ê¸° (ê¸€ì ìˆ˜)
            overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ì—°ê²°ì„± ìœ ì§€ìš©)
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []
        
        chunks = []
        start = 0
        
        while start < len(text):
            # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
            end = start + chunk_size
            
            # í…ìŠ¤íŠ¸ê°€ ë‚¨ì€ ê¸¸ì´ë³´ë‹¤ ì²­í¬ í¬ê¸°ê°€ í¬ë©´ ëê¹Œì§€
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # ë¬¸ì¥ ì¤‘ê°„ì— ëŠì–´ì§€ì§€ ì•Šë„ë¡ ì¡°ì •
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆì„ ì°¾ì•„ì„œ ê±°ê¸°ì„œ ìë¥´ê¸°
            chunk_end = end
            for i in range(end, max(start + chunk_size//2, end - 100), -1):
                if text[i] in '.!?\n':
                    chunk_end = i + 1
                    break
            
            chunks.append(text[start:chunk_end])
            start = chunk_end - overlap  # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì„¤ì •
        
        self.chunks = chunks
        return chunks

    def create_embeddings(self, chunks=None):
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ self.chunks ì‚¬ìš©)
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if chunks is None:
            chunks = self.chunks
            
        embeddings = []
        
        print(f"ì´ {len(chunks)}ê°œ ì²­í¬ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        for i, chunk in enumerate(chunks):
            try:
                # Hugging Face embedding API ì‚¬ìš©
                embedding = self.client.feature_extraction(
                    text=chunk,
                    model="sentence-transformers/all-MiniLM-L6-v2"
                )
                embeddings.append(embedding)
                
                if (i + 1) % 10 == 0:  # 10ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                    print(f"ì§„í–‰ìƒí™©: {i + 1}/{len(chunks)}")
                    
            except Exception as e:
                print(f"ì²­í¬ {i} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë²¡í„°ë¡œ ì±„ìš°ê¸°
                embeddings.append([0] * 384)  # all-MiniLM-L6-v2ëŠ” 384ì°¨ì›
        
        print("ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        self.embeddings = embeddings
        return embeddings

    def search_similar_chunks(self, query, top_k=3, show_preview=True, show_full_text=False):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ PDF ì²­í¬ë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            show_preview: ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ ì—¬ë¶€
            show_full_text: ê²€ìƒ‰ëœ ì „ì²´ ë¬¸ë‹¨ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë“¤ê³¼ ìœ ì‚¬ë„ ì ìˆ˜
        """
        if not self.chunks or not self.embeddings:
            print("PDF ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”.")
            return []
            
        print(f"ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘: {query}")
        
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        try:
            query_embedding = self.client.feature_extraction(
                text=query,
                model="sentence-transformers/all-MiniLM-L6-v2"
            )
        except Exception as e:
            print(f"ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []
        
        # 2. ì§ˆë¬¸ ì„ë² ë”©ê³¼ ëª¨ë“  ì²­í¬ ì„ë² ë”© ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        query_embedding = np.array(query_embedding).reshape(1, -1)
        chunk_embeddings = np.array(self.embeddings)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
        
        # 3. ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_indices = np.argsort(similarities)[::-1][:top_k]
        
        # 4. ê²°ê³¼ ë°˜í™˜
        results = []
        for i, idx in enumerate(similar_indices):
            results.append({
                'rank': i + 1,
                'chunk': self.chunks[idx],
                'similarity': similarities[idx],
                'chunk_index': idx
            })
        
        # 5. ê°„ë‹¨í•œ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ (ì˜µì…˜)
        if show_preview:
            print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ Top {top_k}:")
            for result in results:
                score = result['similarity']
                if show_full_text:
                    # ì „ì²´ ë¬¸ë‹¨ ì¶œë ¥
                    print(f"\nğŸ† {result['rank']}ìœ„ (ìœ ì‚¬ë„: {score:.3f})")
                    print(f"ğŸ“ ì²­í¬ ì¸ë±ìŠ¤: {result['chunk_index']}")
                    print(f"ğŸ“ ì „ì²´ ë‚´ìš©:")
                    print("=" * 80)
                    print(result['chunk'])
                    print("=" * 80)
                else:
                    # ë¯¸ë¦¬ë³´ê¸°ë§Œ ì¶œë ¥ (ê¸°ì¡´ ë°©ì‹)
                    preview = result['chunk'][:100] + "..." if len(result['chunk']) > 100 else result['chunk']
                    print(f"  {result['rank']}ìœ„ (ìœ ì‚¬ë„: {score:.3f}) - {preview}")
            print()
        
        print(f"âœ… ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ {top_k}ê°œ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results
    
    def process_pdf(self, pdf_path, chunk_size=1000, overlap=200):
        """
        PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ë¶„í•  â†’ ì„ë² ë”©)
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
            overlap: ì²­í¬ ê²¹ì¹¨
        
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€ (bool)
        """
        print("=== PDF ì²˜ë¦¬ ì‹œì‘ ===")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            return False
        
        # 2. í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = self.chunk_text(text, chunk_size, overlap)
        print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        # 3. ì„ë² ë”© ìƒì„±
        embeddings = self.create_embeddings(chunks)
        print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        
        print("=== PDF ì²˜ë¦¬ ì™„ë£Œ ===")
        return True
    
    def process_pdf_folder(self, folder_path, chunk_size=1000, overlap=200):
        """
        í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ìºì‹± ì§€ì›)
        
        Args:
            folder_path: PDF íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
            overlap: ì²­í¬ ê²¹ì¹¨
        
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€ (bool)
        """
        print(f"=== í´ë” ë‚´ PDF ì²˜ë¦¬ ì‹œì‘: {folder_path} ===")
        
        # 1. ìºì‹œ í™•ì¸
        cache_loaded, cached_chunks, cached_embeddings = self._load_cache(folder_path)
        
        if cache_loaded:
            print("ğŸš€ ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)")
            self.chunks = cached_chunks
            self.embeddings = cached_embeddings
            return True
        
        # 2. ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì²˜ë¦¬
        print("â³ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        # PDF íŒŒì¼ ëª©ë¡ ì°¾ê¸°
        pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
        
        if not pdf_files:
            print("í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        for pdf_file in pdf_files:
            print(f"  - {os.path.basename(pdf_file)}")
        
        all_text = ""
        processed_count = 0
        
        # ê° PDF íŒŒì¼ ì²˜ë¦¬
        for pdf_file in pdf_files:
            print(f"\nì²˜ë¦¬ ì¤‘: {os.path.basename(pdf_file)}")
            text = self.extract_text_from_pdf(pdf_file)
            
            if text:
                all_text += f"\n\n=== {os.path.basename(pdf_file)} ===\n"
                all_text += text
                processed_count += 1
            else:
                print(f"ì‹¤íŒ¨: {os.path.basename(pdf_file)}")
        
        if not all_text:
            print("ì²˜ë¦¬ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ PDF: {processed_count}ê°œ")
        
        # í†µí•© í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = self.chunk_text(all_text, chunk_size, overlap)
        print(f"í†µí•© í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.create_embeddings(chunks)
        print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        
        # 3. ìºì‹œ ì €ì¥
        self._save_cache(folder_path, chunks, embeddings)
        
        print("=== í´ë” ë‚´ PDF ì²˜ë¦¬ ì™„ë£Œ ===")
        return True